# MOIS_3_ASYNCHRONE_OUTILLAGE/SEMAINE_4_Scraper_Projet.md : SEMAINE 4 : Projet â€“ Scraper Web Asynchrone

## ğŸ¯ Objectif du Projet Final
Construire un scraper capable de rÃ©cupÃ©rer des informations sur plusieurs pages web en parallÃ¨le, avec gestion des erreurs, logging et configuration par environnement.

---

## ğŸ—ï¸ Architecture du Projet

Le projet sera structurÃ© ainsi :
```bash
Projet_Scraper/
â”œâ”€â”€ .env                # ClÃ©s et configuration
â”œâ”€â”€ .gitignore          # Exclusion du .env
â”œâ”€â”€ app.log             # Historique des opÃ©rations
â”œâ”€â”€ main.py             # CÅ“ur du programme
â””â”€â”€ requirements.txt    # httpx, python-dotenv
```

---

## ğŸš€ Ã‰tape 1 : Configuration (30 min)

**Objectif :** PrÃ©parer l'environnement.

#### `requirements.txt`
```text
httpx
python-dotenv
```

#### `.env`
```text
BASE_URL=https://jsonplaceholder.typicode.com/posts
MAX_CONCURRENT_REQUESTS=5
```

---

## ğŸ› ï¸ Ã‰tape 2 : Le Code Asynchrone (1h30)

Nous allons utiliser `httpx`, une bibliothÃ¨que moderne qui remplace `requests` pour l'asynchronisme.

#### ğŸ“ Code GuidÃ© : `main.py`

```python
import asyncio
import httpx
import logging
import os
from dotenv import load_dotenv
from functools import wraps
import time

# --- LOGGING CONFIG ---
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# --- DECORATEUR DE TIMING ---
def chrono(func):
    @wraps(func)
    async def wrapper(*args, **kwargs):
        start = time.perf_counter()
        res = await func(*args, **kwargs)
        end = time.perf_counter()
        logger.info(f"â±ï¸ {func.__name__} a pris {end - start:.2f}s")
        return res
    return wrapper

# --- LOGIQUE DE SCRAPING ---
async def fetch_post(client, post_id):
    url = f"{os.getenv('BASE_URL')}/{post_id}"
    try:
        response = await client.get(url)
        response.raise_for_status()
        data = response.json()
        logger.info(f"âœ… Post {post_id} rÃ©cupÃ©rÃ© : {data['title'][:20]}...")
        return data
    except Exception as e:
        logger.error(f"âŒ Erreur sur le post {post_id} : {e}")
        return None

@chrono
async def main():
    load_dotenv()
    ids = range(1, 11) # On rÃ©cupÃ¨re les 10 premiers posts
    
    async with httpx.AsyncClient() as client:
        tasks = [fetch_post(client, i) for i in ids]
        resultats = await asyncio.gather(*tasks)
    
    valides = [r for r in resultats if r is not None]
    print(f"\nğŸš€ Total rÃ©cupÃ©rÃ© : {len(valides)} / {len(ids)}")

if __name__ == "__main__":
    asyncio.run(main())
```

---

## ğŸ§  Deep Dive : Session vs RequÃªtes isolÃ©es (30 min)

> [!IMPORTANT]
> **Pourquoi `httpx.AsyncClient()` ?**  
> Utiliser un client dans un bloc `async with` permet de rÃ©utiliser les connexions TCP (Keep-Alive). Si vous crÃ©iez un nouveau client pour chaque requÃªte, votre programme serait beaucoup plus lent et pourrait Ãªtre banni par le serveur.

---

## ğŸ§ª DÃ‰FIS SUPPLÃ‰MENTAIRES (Pour les experts)

### DÃ©fi 1 : Limitation de DÃ©bit (Rate Limiting)
Modifiez le code pour utiliser un `asyncio.Semaphore`. Cela permet de ne pas lancer 1000 requÃªtes d'un coup, mais par exemple seulement 5 Ã  la fois.

### DÃ©fi 2 : Sauvegarde en fichier
Ajoutez une fonction qui sauvegarde les rÃ©sultats dans un fichier `data.json` Ã  la fin de l'exÃ©cution.

---

## â³ Conclusion du Mois 3

Vous avez maintenant entre les mains le pouvoir de l'asynchronisme. Cette compÃ©tence est ce qui diffÃ©rencie un dÃ©veloppeur junior d'un dÃ©veloppeur capable de gÃ©rer des systÃ¨mes Ã  haute performance.

  * **Revue :** Quels sont les avantages de `httpx` sur `requests` ? 
  * **Prochain Mois :** Nous allons utiliser toutes ces connaissances pour crÃ©er de vraies APIs avec **FastAPI** !
